ğŸš€ Task 1 â€” BERT: Customer Feedback Sentiment Classification
Problem

Classify customer feedback into Positive, Negative, or Neutral sentiments.

Dataset

Kaggle: Customer Feedback Sentiment Dataset
(https://www.kaggle.com/datasets/vishweshsalodkar/customer-feedback-dataset
)

Deliverables

âœ” Preprocessing + Tokenization
âœ” Training + Validation Pipeline
âœ” Evaluation Metrics
âœ” Example Predictions

Evaluation Metrics

Accuracy

Precision / Recall / F1-Score

Confusion Matrix

ğŸ¤– Task 2 â€” GPT-2/LLaMA: Pseudo-code â†’ Python Code Generation
Problem

Translate structured pseudo-code into syntactically and semantically correct Python code.

Dataset

SPOC (Student Pseudo-code to Code) Dataset
https://github.com/sumith1896/spoc

Research Paper

https://arxiv.org/pdf/1906.04908

Deliverables

âœ” Preprocessing of pseudo-code/code pairs
âœ” Tokenization + Formatting
âœ” Fine-tuning GPT-2 (Causal LM)
âœ” Evaluation Metrics:
â€“ BLEU
â€“ CodeBLEU
â€“ Human Evaluation
âœ” Streamlit / Gradio Interface

Output Example

User enters pseudo-code â†’ Model returns working Python code.

ğŸ“ Task 3 â€” T5/BART: Abstractive Text Summarization
Problem

Generate concise summaries from long news articles.

Dataset

Kaggle (CNN-DailyMail Summarization Dataset)
https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail

Deliverables

âœ” Dataset Preprocessing
âœ” Model Fine-Tuning (T5/BART)
âœ” ROUGE Evaluation
âœ” Example Summaries

Evaluation Metrics

ROUGE-1

ROUGE-2

ROUGE-L

Human evaluation for readability + relevance

ğŸ›  Installation
git clone <repo-url>
cd project-folder
pip install -r requirements.txt

â–¶ï¸ How to Run Each Task
Task 1 (BERT)
cd Task1_BERT_Sentiment
python preprocess.py
python train.py
python evaluate.py

Task 2 (GPT-2/LLaMA)
cd Task2_GPT_Pseudocode2Code
python preprocess_pairs.py
python finetune_gpt2.py
python evaluate_metrics.py
streamlit run app.py

Task 3 (T5/BART)
cd Task3_T5_Summarization
python preprocess.py
python finetune_t5.py
python evaluate_summarizer.py

ğŸ“Š Results

Each task folder contains:

Metrics report

Model checkpoints

Example outputs

Plots (accuracy, loss curves, ROUGE, BLEU, etc.)

ğŸ™Œ Acknowledgements

BERT, GPT-2, T5, BART â€” Hugging Face Transformers

Kaggle + SPOC Dataset creators

Research papers referenced in each task
